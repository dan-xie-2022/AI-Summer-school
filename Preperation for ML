Data cleaning
### posts about the steps in data cleaning
https://www.kaggle.com/search?q=data+cleaning
### notebooks on kaggle for data cleaning
https://www.kaggle.com/code/fedi1996/house-prices-data-cleaning-viz-and-modeling
# This one has some special sections: feature selection, normality check, convert strings to categorical values via LabelEncoder, correlation matrix, feature importance, dimensionality reduction
# This one also has many tips for visualization of the data cleaning process
https://www.kaggle.com/code/code1110/houseprice-data-cleaning-visualization
#textual data
https://www.kaggle.com/code/ragnisah/text-data-cleaning-tweets-analysis

###some specific functions:
#Scale and Normalization# https://www.kaggle.com/code/rtatman/data-cleaning-challenge-scale-and-normalize-data
#Date data# https://www.kaggle.com/code/danxie229/data-cleaning-challenge-parsing-dates/edit
#Encoding data (binary to readable)# https://www.kaggle.com/code/rtatman/data-cleaning-challenge-character-encodings/notebook


Data exploration 
### A comprehensive data explaration:::https://www.kaggle.com/code/pmarcelino/comprehensive-data-exploration-with-python/notebook
### What is data exploration:
#- Discovering early patterns in the data.
#- Understand the first relationships of the variables.
#- Initial analysis to discover where to go from here. 

### Why is it important? 
#- Simplifies future analysis.
#- Guides data analysis.
#- Clean up data by removing unnecessary data. 

### Reporting NA
e.g. 
#define the function to calculate NA in a columns in a dataframe
def display_missing(df):    
    for col in df.columns.tolist():          
        print('{} column missing values: {}'.format(col, df[col].isnull().sum()))
    print('\n')
#define the function to use the above function to every dataset in a set of multiple dataset
for df in dfs:
    print('{}'.format(df.name))
    display_missing(df)
    


#Summarize data across groups(version 1)
Pandas: dataset.groupby()/dataset.pivot_table()
#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html
Two factors: dataset.pivote_table('DV', index='IV1', columns='IV2')
Multiple factors: dataset.pivot_table('DV', ['IV1_1', IV1_2], 'IV2')
Multiple DV: dataset.pivot_table(index='IV1', columns='IV2', aggfunc={'DV1':sum, 'DV2':'mean'})
# more infomation: https://jakevdp.github.io/PythonDataScienceHandbook/03.09-pivot-tables.html
$ pandans.cut 实现分组


### summerize across groups(version 2)
# .groupby/.pivote_table # more information here, it also has #Multi-level pivot tables
https://jakevdp.github.io/PythonDataScienceHandbook/03.09-pivot-tables.html
e.g.1 dataset.pivot_table('index', ['group1_character', Group1_categorical], ['dv1', dv2])
e.g.2 dataset.pivot_table(index='sex', columns='class', aggfunc={'survived':sum, 'fare':'mean'})
##There is also a pandas.pivot_table
##The two examples are a bit different, need to be careful when using
##panda.cut seperate a variables into categories/intervals 

###using heatmap to visualize the summeriz across group is more efficient 
seaborn.heatmap
### For continous variable, using histgram to visualize data is more efficient
seaborn.distplot/displot/hisplot
#see more:https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751


#Box plot are perfect to detect outliers and comapre differents categories
e.g.
f, ax = plt.subplots(figsize=(13, 7))

seaborns.boxplot(data=dataset, x='primary_use', y='log_meter_reading', ax=ax)
ax.set_ylabel('log meter reading (kWh)')
ax.set_xlabel('Primary usage')
ax.set_xticklabels(ax.get_xticklabels(), rotation=90)

_ = f.suptitle('Meter reading by primary_use', fontsize=20)

#Geo data figure can put data on a map, very useful to visualize geo-data
e.g.

Step 1: Get the median consumption for each site (while keeping geo position): 

site_median_consumption = (
    dataset.groupby(['site_id', 'lat', 'lng'])
    ['log_meter_reading'].median()
    .reset_index()
)

Step 2: Plot site median consumption on a map:
fig = px.scatter_mapbox(
    site_median_consumption,
    lat="lat", lon="lng",
    hover_name="site_id",
    color='log_meter_reading', size='log_meter_reading',
    color_discrete_sequence=["fuchsia"], zoom=2, height=300
)
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r": 0, "t": 0, "l": 0, "b": 0})
fig.show()

#visualize a time series
Step 1. Create a numerical variable from date format data
# create datetime related features to simplify the analysis
dataset['meter_name'] = dataset.meter.map(dict_meter_name)
dataset['day'] = dataset.timestamp.dt.day.astype(np.int8)
dataset['month'] = dataset.timestamp.dt.month.astype(np.int8)
dataset['hour'] = dataset.timestamp.dt.hour.astype(np.int8)
dataset['day_of_week'] = dataset.timestamp.dt.dayofweek.astype("int8")

Step 2. map it
meters_by_hour = (
    dataset
    .groupby('hour')
    .agg({'log_meter_reading': 'median'})
)

fig, ax = plt.subplots(figsize=(11, 5))
sns.lineplot(data=meters_by_hour, x="hour", y="log_meter_reading", ax=ax)
_ = ax.set(title="Time of Day's Meter Reading Distribution")


#visualize correlation heatmap
e.g.

Step 1. define the function

def plot_weather_correlations(data):
    """Plot the correlations between weather data
    and the energy consumption.
    1. select the colum
    2. get the correlation coefficient
    3. plot the figure
    """

    columns_to_show = [
        "log_meter_reading",
        "air_temperature",
        "cloud_coverage",
        "dew_temperature",
        "precip_depth_1_hr",
        "sea_level_pressure",
        "wind_direction",
        "wind_speed"
    ]

    correlations = (
        data
        [columns_to_show]
        .corr(method="spearman")
    )

    fig, ax = plt.subplots(1, 1, figsize=(10, 9))
    _ = sns.heatmap(
        correlations,
        cmap="coolwarm",
        vmin=-1,
        vmax=1,
        annot=True,
        ax=ax,
    )
    plt.xticks(rotation=45)
    _ = plt.show()

Step 2. apply the function

plot_weather_correlations(dataset)

Feature engineering

It is divided into 3 broad categories:

Feature Selection: All features aren't equal. It is all about selecting a small subset of features from a large pool of features. 
We select those attributes which best explain the relationship of an independent variable with the target variable. 
There are certain features which are more important than other features to the accuracy of the model. It is different 
from dimensionality reduction because the dimensionality reduction method does so by combining existing attributes, 
whereas the feature selection method includes or excludes those features. The methods of Feature Selection are 
Chi-squared test, correlation coefficient scores, LASSO, Ridge regression etc.

Feature Transformation: It means transforming our original feature to the functions of original features. Scaling, discretization,
binning and filling missing data values are the most common forms of data transformation. To reduce right skewness of the data, we use log.

Feature Extraction: When the data to be processed through an algorithm is too large, it’s generally considered redundant. 
Analysis with a large number of variables uses a lot of computation power and memory, therefore we should reduce the dimensionality of these types of variables. 
It is a term for constructing combinations of the variables. For tabular data, we use PCA to reduce features. For image, we can use line or edge detection.



It is always a good practice to explicitly state the features that we would like to use before training machine learning algorithms.

e.g.
columns_to_use = [
 'Vi', 'V2',...'Vn'
]
data = data[columns_to_use]

Feature engineering is the process by which knowledge of data is used to construct explanatory variables, features, that can be used to 
train a predictive model. Engineering and selecting the correct features for a model will not only significantly improve its predictive power, 
but will also offer the flexibility to use less complex models that are faster to run and more easily understood.

#Feature Engineering from transformation
#Feature transformations can include aggregating, combining transforming attributes to create new features. Useful and relevant features 
will depend on the problem at hand but averages, sums, log or ratios can better expose trends to a model.

#We can also transform a numerical feature into a categorical feature by cutting it into classes. This can be interesting to avoid the impact 
of outliers or to reduce the variance of the output variable.

Example:
# log transformation
data['square_feet_log'] = data['square_feet'].apply(np.log)

# polynomial transformation
data['air_temperature_squared'] = data['air_temperature']**2

#Some algorithms can't work with categorical data directly. This means that categorical data must be converted to a numerical form. 
To Convert Categorical Data to Numerical Data, this involves two steps :

Integer (ordinal or cardinal): assign value to each category
One-Hot Encoding: Create dummy variables ### Use pd.get_dummies() for OneHotEncoding ###You can also use scikitlearn library for OneHotEnding(https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)

e.g. how to encode to one hot encoding
# list of columns to encode using One-Hot-Encoding
columns_to_encode = ["meter_name", "primary_use", "zone_geo", "site_id"]

# encode those columns
encoded_data = pd.get_dummies(data[columns_to_encode], columns=columns_to_encode)

# add encoded columns to the data
data = pd.concat([data, encoded_data], axis=1) ###{for axis 0/’index’, 1/’columns’}, default 0}




Data split to: training, validation and test

The **train-validation-test** split is a technique for training and evaluating the performance of a machine learning algorithm.

The **procedure** involves taking a dataset and dividing it into three subsets:
- The **train** subset is used to fit the model and is referred to as the training dataset. You should not evaluate the performance of the model on this train set,
- The **validation** subset is used to tune hyperparameters of an algorithm. For example the *max_depth* for a regression tree,
- The **test** subset is not used to train the model; it is only used at the end to evaluate the performances of the model. For a purist, once you 'opened' the test set, you should not modify the model anymore.


**Strategy**
   - Define a test set, and separate the remaining between train and validation.
   - Generally they represent respectively 70% | 15% | 15% of the initial dataset.
### dataset1_v1.isin(V2) #check if dataset1_v1 is in V2


Splitting your data (Features / Target)
It is a rather practical approach because generally the algorithms of machine learning ask for the features on the one hand and the target on the other hand.

Step 1: define the feature and the target/label e.g. features = ['V1', 'V2'...'Vn']; target = 'Vt'
Step 2: select the feature subset and the target subset e.g. features = dataset[features], target = dataset[target]


Scaling of Data

Depending on the task at hand, it can be useful to scale each feature by subtracting the mean and dividing it by its standard deviation.
In practice we often use scikit-learn's standard scaler objects. Note that the test set is scaled with the means and standard deviations 
calculated on the columns of the training set
e.g. scale function from sklearn.preprocessing import StandardScaler ### scaler.transform(data) data can be changed flexabily

##Some practical issue in data scale

Data scaling is mainly used to speed up model fitting or improving the performance of some model families such as neural networks or support vector machines.

Other models are not sensitive to scaling like random forests.

In addition to this, scaling data allows to compare the coefficients of a linear model trained on the data and to determine which variables are the most 
important for the prediction, based on the absolute value of their coefficient.


Evaluation Metrics for a Regression Problem
 In this lab, as we are interested in a regression problem, we will first see some classic regression metrics.

Suppose we evaluate these metrics for a set $(y_i, \hat{y}_i)_{i=1,...n_{\text{test}}}$, where $y_i$ is the true value and   $\hat{y_i}$ is the prediction.


- **mean absolute error**:   
$\text{MAE} = \frac{1}{n_{\text{test}}} \sum_{i=1,...n_{\text{test}}} |y_i - \hat{y_i}|$ .

- **mean squared error**:   
$\text{MSE} = \frac{1}{n_{\text{test}}} \sum_{i=1,...n_{\text{test}}} (y_i - \hat{y_i})^2$. The most used.

- **max error**: 
$\text{MAX_Error} =  \max_{i=1,...n_{\text{test}}} (y_i - \hat{y_i})$. Calculates the maximum residual error. It is very sensitive to outliers

MSE has the benefit of penalizing large errors more so can be more appropriate in some cases, for example, if being off by 10 is more than twice as bad as being off by 5.

From an interpretation standpoint, MAE is clearly the winner. MSE does not describe average error alone and has other implications that are more difficult to tease out and understand.


Fit a Linear Model on the data

###code to run the regression
reg = linear_model.LinearRegression()
reg = reg.fit(train_features, train_target)

Check the Feature importances
import matplotlib.pyplot as plt

coefs = pd.DataFrame(
   reg.coef_,
   columns=['Coefficients'], index=train_features.columns
)

coefs.plot(kind='barh', figsize=(9, 7))
plt.title('Linear model')
plt.axvline(x=0, color='.5')
plt.subplots_adjust(left=.3)

## Evaluation of predictions








