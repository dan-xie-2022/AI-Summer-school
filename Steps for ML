Data cleaning
### posts about the steps in data cleaning
https://www.kaggle.com/search?q=data+cleaning
### notebooks on kaggle for data cleaning
https://www.kaggle.com/code/fedi1996/house-prices-data-cleaning-viz-and-modeling
# This one has some special sections: feature selection, normality check, convert strings to categorical values via LabelEncoder, correlation matrix, feature importance, dimensionality reduction
# This one also has many tips for visualization of the data cleaning process
https://www.kaggle.com/code/code1110/houseprice-data-cleaning-visualization
#textual data
https://www.kaggle.com/code/ragnisah/text-data-cleaning-tweets-analysis

###some specific functions:
#Scale and Normalization# https://www.kaggle.com/code/rtatman/data-cleaning-challenge-scale-and-normalize-data
#Date data# https://www.kaggle.com/code/danxie229/data-cleaning-challenge-parsing-dates/edit
#Encoding data (binary to readable)# https://www.kaggle.com/code/rtatman/data-cleaning-challenge-character-encodings/notebook


Data exploration 
### What is data exploration:
#- Discovering early patterns in the data.
#- Understand the first relationships of the variables.
#- Initial analysis to discover where to go from here. 

### Why is it important? 
#- Simplifies future analysis.
#- Guides data analysis.
#- Clean up data by removing unnecessary data. 

#Summarize data across groups(version 1)
Pandas: dataset.groupby()/dataset.pivot_table()
#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html
Two factors: dataset.pivote_table('DV', index='IV1', columns='IV2')
Multiple factors: dataset.pivot_table('DV', ['IV1_1', IV1_2], 'IV2')
Multiple DV: dataset.pivot_table(index='IV1', columns='IV2', aggfunc={'DV1':sum, 'DV2':'mean'})
# more infomation: https://jakevdp.github.io/PythonDataScienceHandbook/03.09-pivot-tables.html
$ pandans.cut 实现分组


### summerize across groups(version 2)
# .groupby/.pivote_table # more information here, it also has #Multi-level pivot tables
https://jakevdp.github.io/PythonDataScienceHandbook/03.09-pivot-tables.html
e.g.1 dataset.pivot_table('index', ['group1_character', Group1_categorical], ['dv1', dv2])
e.g.2 dataset.pivot_table(index='sex', columns='class', aggfunc={'survived':sum, 'fare':'mean'})
##There is also a pandas.pivot_table
##The two examples are a bit different, need to be careful when using
##panda.cut seperate a variables into categories/intervals 

###using heatmap to visualize the summeriz across group is more efficient 
seaborn.heatmap
### For continous variable, using histgram to visualize data is more efficient
seaborn.distplot/displot/hisplot
#see more:https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751


#Box plot are perfect to detect outliers and comapre differents categories
e.g.
f, ax = plt.subplots(figsize=(13, 7))

seaborns.boxplot(data=dataset, x='primary_use', y='log_meter_reading', ax=ax)
ax.set_ylabel('log meter reading (kWh)')
ax.set_xlabel('Primary usage')
ax.set_xticklabels(ax.get_xticklabels(), rotation=90)

_ = f.suptitle('Meter reading by primary_use', fontsize=20)

#Geo data figure can put data on a map, very useful to visualize geo-data
e.g.

Step 1: Get the median consumption for each site (while keeping geo position): 

site_median_consumption = (
    dataset.groupby(['site_id', 'lat', 'lng'])
    ['log_meter_reading'].median()
    .reset_index()
)

Step 2: Plot site median consumption on a map:
fig = px.scatter_mapbox(
    site_median_consumption,
    lat="lat", lon="lng",
    hover_name="site_id",
    color='log_meter_reading', size='log_meter_reading',
    color_discrete_sequence=["fuchsia"], zoom=2, height=300
)
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r": 0, "t": 0, "l": 0, "b": 0})
fig.show()

#visualize a time series
Step 1. Create a numerical variable from date format data
# create datetime related features to simplify the analysis
dataset['meter_name'] = dataset.meter.map(dict_meter_name)
dataset['day'] = dataset.timestamp.dt.day.astype(np.int8)
dataset['month'] = dataset.timestamp.dt.month.astype(np.int8)
dataset['hour'] = dataset.timestamp.dt.hour.astype(np.int8)
dataset['day_of_week'] = dataset.timestamp.dt.dayofweek.astype("int8")

Step 2. map it
meters_by_hour = (
    dataset
    .groupby('hour')
    .agg({'log_meter_reading': 'median'})
)

fig, ax = plt.subplots(figsize=(11, 5))
sns.lineplot(data=meters_by_hour, x="hour", y="log_meter_reading", ax=ax)
_ = ax.set(title="Time of Day's Meter Reading Distribution")


#visualize correlation heatmap
e.g.

Step 1. define the function

def plot_weather_correlations(data):
    """Plot the correlations between weather data
    and the energy consumption.
    1. select the colum
    2. get the correlation coefficient
    3. plot the figure
    """

    columns_to_show = [
        "log_meter_reading",
        "air_temperature",
        "cloud_coverage",
        "dew_temperature",
        "precip_depth_1_hr",
        "sea_level_pressure",
        "wind_direction",
        "wind_speed"
    ]

    correlations = (
        data
        [columns_to_show]
        .corr(method="spearman")
    )

    fig, ax = plt.subplots(1, 1, figsize=(10, 9))
    _ = sns.heatmap(
        correlations,
        cmap="coolwarm",
        vmin=-1,
        vmax=1,
        annot=True,
        ax=ax,
    )
    plt.xticks(rotation=45)
    _ = plt.show()

Step 2. apply the function

plot_weather_correlations(dataset)

Feature engineering

It is always a good practice to explicitly state the features that we would like to use before training machine learning algorithms.

e.g.
columns_to_use = [
 'Vi', 'V2',...'Vn'
]
data = data[columns_to_use]

Feature engineering is the process by which knowledge of data is used to construct explanatory variables, features, that can be used to 
train a predictive model. Engineering and selecting the correct features for a model will not only significantly improve its predictive power, 
but will also offer the flexibility to use less complex models that are faster to run and more easily understood.

#Feature Engineering from transformation
#Feature transformations can include aggregating, combining transforming attributes to create new features. Useful and relevant features 
will depend on the problem at hand but averages, sums, log or ratios can better expose trends to a model.

#We can also transform a numerical feature into a categorical feature by cutting it into classes. This can be interesting to avoid the impact 
of outliers or to reduce the variance of the output variable.

Example:
# log transformation
data['square_feet_log'] = data['square_feet'].apply(np.log)

# polynomial transformation
data['air_temperature_squared'] = data['air_temperature']**2

#Some algorithms can't work with categorical data directly. This means that categorical data must be converted to a numerical form. 
To Convert Categorical Data to Numerical Data, this involves two steps :

Integer (ordinal or cardinal): assign value to each category
One-Hot Encoding: Create dummy variables ### Use pd.get_dummies() for OneHotEncoding ###You can also use scikitlearn library for OneHotEnding(https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)

e.g. how to encode to one hot encoding
# list of columns to encode using One-Hot-Encoding
columns_to_encode = ["meter_name", "primary_use", "zone_geo", "site_id"]

# encode those columns
encoded_data = pd.get_dummies(data[columns_to_encode], columns=columns_to_encode)

# add encoded columns to the data
data = pd.concat([data, encoded_data], axis=1) ###{for axis 0/’index’, 1/’columns’}, default 0}















