XGBOOST
#https://towardsdatascience.com/a-brief-introduction-to-xgboost-3eaee2e3e5d6#:~:text=XGBoost%20vs%20Gradient%20Boosting,can%20be%20parallelized%20across%20clusters.

XGBoost vs Gradient Boosting
XGBoost is a more regularized form of Gradient Boosting. XGBoost uses advanced regularization (L1 & L2), which improves model generalization capabilities.

XGBoost delivers high performance as compared to Gradient Boosting. Its training is very fast and can be parallelized across clusters.

When to use XGBoost?
When there is a larger number of training samples. Ideally, greater than 1000 training samples and less 100 features or we can say when the number of features < number of training samples.
When there is a mixture of categorical and numeric features or just numeric features.

When not to use XGBoost?
Image Recognition
Computer Vision
When the number of training samples is significantly smaller than the number of features.

Does RF AL have the problem of overfitting?
#One opinions: https://mljar.com/blog/random-forest-overfitting/

Model stacking
#blog to introduce the concept: https://medium.com/geekculture/how-to-use-model-stacking-to-improve-machine-learning-predictions-d113278612d4#:~:text=What%20is%20Model%20Stacking%3F,model%20called%20a%20meta%2Dlearner.


Neural networks
### https://www.ibm.com/cloud/learn/neural-networks#:~:text=Neural%20networks%2C%20also%20known%20as,neurons%20signal%20to%20one%20another.
Neural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of machine learning and are 
at the heart of deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons 
signal to one another.

Artificial neural networks (ANNs) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. 
Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above 
the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along 
to the next layer of the network.

Neural networks rely on training data to learn and improve their accuracy over time. However, once these learning algorithms are fine-tuned for 
accuracy, they are powerful tools in computer science and artificial intelligence, allowing us to classify and cluster data at a high velocity. 
Tasks in speech recognition or image recognition can take minutes versus hours when compared to the manual identification by human experts. 
One of the most well-known neural networks is Google’s search algorithm.

Feedforward V.S. Backpropogation
Most deep neural networks are feedforward, meaning they flow in one direction only, from input to output. However, you can also train 
your model through backpropagation; that is, move in the opposite direction from output to input. Backpropagation allows us to calculate 
and attribute the error associated with each neuron, allowing us to adjust and fit the parameters of the model(s) appropriately.

Different types of Neural Network
1. multi-layer perceptrons (MLPs/ANN, ), They are comprised of an input layer, a hidden layer or layers, and an output layer. 
While these neural networks are also commonly referred to as MLPs, it’s important to note that they are actually comprised of sigmoid neurons, 
not perceptrons, as most real-world problems are nonlinear. Data usually is fed into these models to train them, and they are the foundation 
for computer vision, natural language processing, and other neural networks.

2. Convolutional neural networks (CNNs) are similar to feedforward networks, but they’re usually utilized for image recognition, 
pattern recognition, and/or computer vision. These networks harness principles from linear algebra, particularly matrix multiplication, 
to identify patterns within an image.

In other words, it would be expensive. Compared to its predecessors, the main advantage of CNN is that it automatically detects the 
important features without any human supervision. This is why CNN would be an ideal solution to computer vision and image classification problems.
Read more at: https://viso.ai/deep-learning/ann-and-cnn-analyzing-differences-and-similarities/

In general, CNN tends to be a more powerful and 
accurate way of solving classification problems. ANN is still dominant for problems where datasets are limited, and image inputs are not necessary.
Read more at: https://viso.ai/deep-learning/ann-and-cnn-analyzing-differences-and-similarities/



ANN is ideal for solving problems regarding data. Forward-facing algorithms can easily be used to process image data, 
text data, and tabular data. CNN requires many more data inputs to achieve its novel high accuracy rate.
Read more at: https://viso.ai/deep-learning/ann-and-cnn-analyzing-differences-and-similarities/

3. Recurrent neural networks (RNNs) are identified by their feedback loops. These learning algorithms are primarily leveraged when 
using time-series data to make predictions about future outcomes, such as stock market predictions or sales forecasting.

RNN models are widely used in Natural Language Processing (NLP) due to the superiority of processing the data with an input length that is not fixed. 
The task of the AI here is to build a system that can comprehend natural language spoken by humans, e.g., natural language modeling, word embedding, 
and machine translation.
Read more at: https://viso.ai/deep-learning/deep-neural-network-three-popular-types/

For more infromation: 
1. https://viso.ai/deep-learning/deep-neural-network-three-popular-types/#:~:text=Three%20following%20types%20of%20deep,Recurrent%20Neural%20Networks%20(RNN)
2. (demonstration of different network and introduction of when to use it)https://www.analyticsvidhya.com/blog/2020/02/cnn-vs-rnn-vs-mlp-analyzing-3-types-of-neural-networks-in-deep-learning/


Regularization
Two regularization methods for Linear models:  Lasso and Ridge.

Lasso can set some coefficients to zero, thus performing variable selection, while ridge regression cannot.
Both methods allow to use correlated predictors, but they solve multicollinearity issue differently:

In ridge regression, the coefficients of correlated predictors are similar. Ridge works well if there 
are many large parameters of about the same value (when most predictors impact the response).

In lasso, one of the correlated predictors has a larger coefficient, while the rest are (nearly) zero. 
Lasso tends to do well if there are a small number of significant parameters and the others are close to zero

-Lasso is good when you have a few features with high predicting power while the others are useless: 
it will zero out the useless ones and keep only a subset of the variables.

-Ridge is good when the predicting power of your dataset is spread out over the different features: 
it will not zero out features that could be helpful when making predictions but will simply reduce the weight of most variables in the model.

In practice, this is often hard to determine. Thus, the best way is see what is the best MSE you can get on the val set using different values of lambda.

Elastic Net model
Elastic Net first emerged as a result of critique on lasso, whose variable selection can be too dependent on data and thus unstable. 
The solution is to combine the penalties of ridge regression and lasso to get the best of both worlds. Elastic Net aims at minimizing 
the following loss function:

Let's choose an 𝜆1,𝜆2 ∈[0,+inf] :

We want to minimize

𝑚𝑖𝑛𝛽[∑𝑛𝑖=1(𝑦𝑖−𝑋𝑖𝛽)2+𝜆1||𝑋||2+𝜆2||𝑋||22 ]

The most common method to find a good model for a given use case usually consists in:
- training various models on a training set
- comparing their performance on a validation set
- finally, check whether the best models generalize properly on new data by comparing their predictions on a test set, which serves as a common benchmark.

However, this may limitate the comparability of models and lead to erroneous conclusions, as the performance of a given algorithm may depend on the training data, 
and sometimes change even if the training data remains the same (this happens when some model parameters or input are initialized randomly).

In order to mitigate this risk, one might resort to cross-validation:
- the dataset is only divised into two parts: a training and a test set
- the training set is divided into N parts, and we're going to train N distinct models (with identical hyperparameters though)
- each model will be trained on N-1 parts (or folds) of the training set, and its performance will be assessed on the remaining fold
- at the end, we average the performance of the model obtained on the N validation games.


Hyperparameter tunning
#For more info: https://www.kaggle.com/code/pavansanagapati/automated-hyperparameter-tuning

SVM:
Mathmetical explanation: https://www.youtube.com/watch?v=ny1iZ5A8ilA


